{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x640 313.8ms\n",
      "Speed: 2.5ms preprocess, 313.8ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 324.8ms\n",
      "Speed: 1.5ms preprocess, 324.8ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 279.2ms\n",
      "Speed: 1.5ms preprocess, 279.2ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 304.8ms\n",
      "Speed: 2.0ms preprocess, 304.8ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 282.9ms\n",
      "Speed: 0.5ms preprocess, 282.9ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 275.7ms\n",
      "Speed: 1.0ms preprocess, 275.7ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 270.9ms\n",
      "Speed: 1.0ms preprocess, 270.9ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 278.4ms\n",
      "Speed: 1.5ms preprocess, 278.4ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 278.0ms\n",
      "Speed: 1.0ms preprocess, 278.0ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 277.6ms\n",
      "Speed: 1.0ms preprocess, 277.6ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 272.7ms\n",
      "Speed: 1.5ms preprocess, 272.7ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 276.6ms\n",
      "Speed: 1.0ms preprocess, 276.6ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 292.3ms\n",
      "Speed: 0.0ms preprocess, 292.3ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 274.4ms\n",
      "Speed: 1.5ms preprocess, 274.4ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 271.0ms\n",
      "Speed: 1.4ms preprocess, 271.0ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 289.2ms\n",
      "Speed: 1.0ms preprocess, 289.2ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 282.4ms\n",
      "Speed: 0.5ms preprocess, 282.4ms inference, 2.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 267.0ms\n",
      "Speed: 1.0ms preprocess, 267.0ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 294.5ms\n",
      "Speed: 0.5ms preprocess, 294.5ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 269.9ms\n",
      "Speed: 1.5ms preprocess, 269.9ms inference, 2.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 271.0ms\n",
      "Speed: 1.0ms preprocess, 271.0ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 279.3ms\n",
      "Speed: 1.0ms preprocess, 279.3ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 268.5ms\n",
      "Speed: 1.0ms preprocess, 268.5ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 272.0ms\n",
      "Speed: 1.6ms preprocess, 272.0ms inference, 2.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 272.2ms\n",
      "Speed: 1.0ms preprocess, 272.2ms inference, 2.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 268.2ms\n",
      "Speed: 1.0ms preprocess, 268.2ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 273.0ms\n",
      "Speed: 0.5ms preprocess, 273.0ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 267.8ms\n",
      "Speed: 1.5ms preprocess, 267.8ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 272.8ms\n",
      "Speed: 1.0ms preprocess, 272.8ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 273.1ms\n",
      "Speed: 0.0ms preprocess, 273.1ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 271.1ms\n",
      "Speed: 1.0ms preprocess, 271.1ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 275.3ms\n",
      "Speed: 1.6ms preprocess, 275.3ms inference, 0.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 269.2ms\n",
      "Speed: 1.1ms preprocess, 269.2ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 273.5ms\n",
      "Speed: 0.5ms preprocess, 273.5ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 269.6ms\n",
      "Speed: 0.5ms preprocess, 269.6ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 269.2ms\n",
      "Speed: 1.1ms preprocess, 269.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 271.5ms\n",
      "Speed: 1.0ms preprocess, 271.5ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 276.5ms\n",
      "Speed: 1.5ms preprocess, 276.5ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 280.2ms\n",
      "Speed: 1.0ms preprocess, 280.2ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 279.9ms\n",
      "Speed: 1.0ms preprocess, 279.9ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 270.4ms\n",
      "Speed: 1.5ms preprocess, 270.4ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 271.9ms\n",
      "Speed: 1.5ms preprocess, 271.9ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 274.6ms\n",
      "Speed: 1.0ms preprocess, 274.6ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 266.7ms\n",
      "Speed: 1.5ms preprocess, 266.7ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 283.1ms\n",
      "Speed: 0.5ms preprocess, 283.1ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 272.7ms\n",
      "Speed: 1.4ms preprocess, 272.7ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 270.0ms\n",
      "Speed: 0.5ms preprocess, 270.0ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 273.2ms\n",
      "Speed: 1.0ms preprocess, 273.2ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 272.0ms\n",
      "Speed: 1.2ms preprocess, 272.0ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 512x640 271.4ms\n",
      "Speed: 1.0ms preprocess, 271.4ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "    Track_ID  Closing_Line_Length\n",
      "0          1             1.000000\n",
      "1          2             0.000000\n",
      "2          3             1.000000\n",
      "3          4             0.000000\n",
      "4          5             1.414214\n",
      "5          6            10.295630\n",
      "6          7             7.280110\n",
      "7          8             1.414214\n",
      "8          9             0.000000\n",
      "9         10             0.000000\n",
      "10        11             1.414214\n",
      "11        12            11.401754\n",
      "12        13             1.000000\n",
      "13        14             1.000000\n",
      "14        15            16.552945\n",
      "15        16             9.486833\n",
      "16        17             0.000000\n",
      "17        18             0.000000\n",
      "18        19             1.414214\n",
      "19        20             2.236068\n",
      "20        21             1.414214\n",
      "21        22             0.000000\n",
      "22        23             0.000000\n",
      "23        24             1.000000\n",
      "24        25             1.000000\n",
      "25        27             0.000000\n",
      "26        28            11.401754\n",
      "27        34             3.162278\n",
      "28        37             3.000000\n",
      "29        39             1.414214\n",
      "30        40             4.472136\n",
      "31        26             1.414214\n",
      "32        29             0.000000\n",
      "33        42             1.414214\n",
      "34        43             5.000000\n",
      "defaultdict(<class 'float'>, {1: 1.0, 2: 0.0, 3: 1.0, 4: 0.0, 5: 1.4142135623730951, 6: 10.295630140987, 7: 7.280109889280518, 8: 1.4142135623730951, 9: 0.0, 10: 0.0, 11: 1.4142135623730951, 12: 11.40175425099138, 13: 1.0, 14: 1.0, 15: 16.55294535724685, 16: 9.486832980505138, 17: 0.0, 18: 0.0, 19: 1.4142135623730951, 20: 2.23606797749979, 21: 1.4142135623730951, 22: 0.0, 23: 0.0, 24: 1.0, 25: 1.0, 27: 0.0, 28: 11.40175425099138, 34: 3.1622776601683795, 37: 3.0, 39: 1.4142135623730951, 40: 4.47213595499958, 26: 1.4142135623730951, 29: 0.0, 42: 1.4142135623730951, 43: 5.0})\n"
     ]
    }
   ],
   "source": [
    "# OBB\n",
    "import pandas as pd  # Import pandas for easy data handling if you choose to use DataFrame\n",
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from ultralytics import RTDETR\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "#model = YOLO(\"/Users/palbentsen/Desktop/master/obb_Inference/bestYolov8S-obb.mlpackage\")\n",
    "# model = YOLO('/Users/palbentsen/Desktop/master/obb_Inference/bestYolov8S-obb.pt')\n",
    "\n",
    "#model = RTDETR(r\"C:\\Users\\Paal\\Downloads\\bestRefinedAABB.pt\")\n",
    "model = YOLO(r\"C:\\Users\\Paal\\Downloads\\bestRefinedOBB.pt\") #model3\n",
    "\n",
    "# Open the video file\n",
    "video_path = r\"C:\\Users\\Paal\\Downloads\\13.4.A_1sec.avi\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Store the track history\n",
    "\n",
    "track_history = defaultdict(list)\n",
    "cache = {\"frame_count\": 0, \"cache_period\": 5, \"last_results\": None}\n",
    "\n",
    "# Before the loop, initialize a dictionary to store lengths\n",
    "closing_line_lengths = defaultdict(float)\n",
    "\n",
    "# Before the loop, initialize variables for the overall longest line\n",
    "overall_longest_length = 0\n",
    "overall_longest_track_id = None\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        current_frame_longest_length = 0\n",
    "        current_frame_longest_track_id = None\n",
    "\n",
    "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
    "        results = model.track(frame, persist=True, show_conf=False, show_labels=False)\n",
    "\n",
    "        # Get the boxes and track IDs\n",
    "        boxes = results[0].obb.xywhr.cpu()\n",
    "        track_ids = results[0].obb.id.int().cpu().tolist()\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot(labels=False)\n",
    "\n",
    "        # Ensure this part correctly adds points to the track history\n",
    "        for box, track_id in zip(boxes, track_ids):\n",
    "            # Example point addition (ensure this matches your actual logic)\n",
    "            x, y, w, h, r = box\n",
    "            # Assuming x, y represent the center of the box\n",
    "            track_history[track_id].append(\n",
    "                (x, y)\n",
    "            )  # Update this logic as per your actual data structure\n",
    "\n",
    "        # Check each track's closing line length\n",
    "        # we only want to do this every 10 frames\n",
    "        if cap.get(cv2.CAP_PROP_POS_FRAMES) % 10 == 0:\n",
    "            for track_id in track_ids:\n",
    "                if track_history[track_id]:  # Check if the list is not empty\n",
    "                    points = (\n",
    "                        np.hstack(track_history[track_id])\n",
    "                        .astype(np.int32)\n",
    "                        .reshape((-1, 1, 2))\n",
    "                    )\n",
    "                    if len(points) > 1:\n",
    "                        first_point = points[0][0]\n",
    "                        last_point = points[-1][0]\n",
    "                        length = np.linalg.norm(\n",
    "                            np.array(last_point) - np.array(first_point)\n",
    "                        )\n",
    "                        closing_line_lengths[track_id] = length\n",
    "\n",
    "                        # Update the longest line in the current frame\n",
    "                        if length > current_frame_longest_length:\n",
    "                            current_frame_longest_length = length\n",
    "                            current_frame_longest_track_id = track_id\n",
    "\n",
    "                        # Update the overall longest line seen so far\n",
    "                        if length > overall_longest_length:\n",
    "                            overall_longest_length = length\n",
    "                            overall_longest_track_id = track_id\n",
    "\n",
    "        # Before displaying the annotated frame\n",
    "        for track_id in track_ids:\n",
    "            # Check if the list is not empty and contains items that can be stacked\n",
    "            if track_history[track_id] and all(\n",
    "                isinstance(point, (np.ndarray, list, tuple))\n",
    "                for point in track_history[track_id]\n",
    "            ):\n",
    "                points = (\n",
    "                    np.hstack(track_history[track_id])\n",
    "                    .astype(np.int32)\n",
    "                    .reshape((-1, 1, 2))\n",
    "                )\n",
    "                if len(points) > 1:\n",
    "                    color = (0, 255, 255)  # Default color for non-longest tracks\n",
    "                    thickness = 1  # Default thickness for non-longest tracks\n",
    "                    if track_id == overall_longest_track_id:\n",
    "                        # Use a distinct color and thickness for the longest track\n",
    "                        color = (0, 255, 255)  # Highlight color for the longest track\n",
    "                        thickness = 1  # Increased thickness for the longest track\n",
    "\n",
    "                        # Draw the polyline for the track\n",
    "                        cv2.polylines(\n",
    "                            annotated_frame,\n",
    "                            [points],\n",
    "                            isClosed=False,\n",
    "                            color=color,\n",
    "                            thickness=thickness,\n",
    "                        )\n",
    "\n",
    "                        # Add text annotation\n",
    "                        text_position = points[-1][\n",
    "                            0\n",
    "                        ]  # Position the text at the end of the track\n",
    "                        cv2.putText(\n",
    "                            annotated_frame,\n",
    "                            \"Best swimmer\",\n",
    "                            (text_position[0] + 10, text_position[1] + 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.6,\n",
    "                            (120, 120, 255),\n",
    "                            2,\n",
    "                        )\n",
    "                    else:\n",
    "                        # Draw the polyline for non-winning tracks\n",
    "                        cv2.polylines(\n",
    "                            annotated_frame,\n",
    "                            [points],\n",
    "                            isClosed=False,\n",
    "                            color=color,\n",
    "                            thickness=thickness,\n",
    "                        )\n",
    "\n",
    "                    first_point, last_point = points[0][0], points[-1][0]\n",
    "                    if track_id == overall_longest_track_id:\n",
    "                        # Highlight the closing line of the longest track differently\n",
    "                        cv2.line(\n",
    "                            annotated_frame,\n",
    "                            tuple(first_point),\n",
    "                            tuple(last_point),\n",
    "                            color=(255, 0, 255),\n",
    "                            thickness=2,\n",
    "                        )\n",
    "            else:\n",
    "                # Skip this track_id if the history is empty or not properly formatted\n",
    "                continue\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Tracking with highlighted winner: Video 82\", annotated_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Convert the lengths to a DataFrame for easy handling\n",
    "lengths_df = pd.DataFrame(\n",
    "    list(closing_line_lengths.items()), columns=[\"Track_ID\", \"Closing_Line_Length\"]\n",
    ")\n",
    "print(lengths_df)\n",
    "\n",
    "# Or simply print the dictionary if you prefer\n",
    "print(closing_line_lengths)\n",
    "\n",
    "# Processing after loop completion\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forAnything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
